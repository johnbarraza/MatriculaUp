---
phase: 01-extraction-pipeline-fix-validation
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - tests/conftest.py
  - tests/fixtures/sample_rows.py
  - tests/test_extraction.py
  - tests/test_validation.py
  - requirements.txt
autonomous: true
requirements: [EXT-02, EXT-03]

must_haves:
  truths:
    - "pytest tests/test_extraction.py -x exits 0 with 5+ passing tests before any implementation code is written (tests fail RED first, then pass GREEN after extraction code is added)"
    - "test_prerequisite_continuation fails RED when fed a truncated prerequisite string"
    - "test_professor_spanish_names fails RED when fed a naive regex that stops at lowercase prepositions"
    - "test_json_schema_compliance fails RED when fed an incomplete course dict"
  artifacts:
    - path: "tests/conftest.py"
      provides: "shared fixtures — sample table rows and PDF mock helpers"
    - path: "tests/fixtures/sample_rows.py"
      provides: "hard-coded sample rows matching actual PDF format"
    - path: "tests/test_extraction.py"
      provides: "unit tests for prerequisite continuation buffer and Spanish name regex"
    - path: "tests/test_validation.py"
      provides: "jsonschema compliance tests for courses and curriculum output dicts"
    - path: "requirements.txt"
      provides: "pytest + pytest-cov added as dev dependencies"
  key_links:
    - from: "tests/conftest.py"
      to: "tests/test_extraction.py"
      via: "pytest fixtures imported automatically"
      pattern: "def sample_table_rows"
    - from: "tests/test_extraction.py"
      to: "scripts/extractors/courses.py"
      via: "import that does not yet exist — tests MUST fail RED before implementation"
      pattern: "from scripts.extractors"
---

<objective>
Create the test scaffolding (Wave 0 / TDD Red phase) that implementation plans 02 and 03 must pass.

Purpose: Nyquist rule requires automated test commands to exist before implementation tasks reference them. Test failures provide the RED baseline that proves implementation actually fixed the bugs.
Output: pytest test files that fail predictably until extraction code is written.
</objective>

<execution_context>
@C:/Users/johnb/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/johnb/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/01-extraction-pipeline-fix-validation/01-CONTEXT.md
@.planning/phases/01-extraction-pipeline-fix-validation/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test fixtures and conftest</name>
  <files>
    tests/conftest.py
    tests/fixtures/sample_rows.py
    requirements.txt
  </files>
  <action>
Create `tests/` directory structure with conftest and fixture data.

`requirements.txt` — add if not present (do NOT overwrite existing deps, only append missing):
```
pdfplumber>=0.11.8
pandas>=2.3.3
jsonschema>=4.20.0
pytest>=8.0.0
pytest-cov>=5.0.0
```

`tests/fixtures/sample_rows.py` — hard-coded sample rows that mimic real PDF table rows for the 2026-1 Economía offer. Include:
- `COURSE_HEADER_ROW`: a list representing one row with 6-digit course code, course name, credits
- `PREREQ_ROW_TRUNCATED`: a continuation row where first cell is empty and text ends with "Y ("
- `PREREQ_ROW_CONTINUATION`: follow-up rows that complete the prerequisite expression
- `SECTION_ROW_CLASE`: a row representing a CLASE session with section letter, professor, day, time range, room
- `PROFESSOR_COMPOUND_ROW`: a row containing "CASTROMATTA, Milagros Del Rosario / GARCIA, Juan De La Cruz" as professor cell text

`tests/conftest.py`:
```python
import pytest
from tests.fixtures.sample_rows import (
    COURSE_HEADER_ROW, PREREQ_ROW_TRUNCATED, PREREQ_ROW_CONTINUATION,
    SECTION_ROW_CLASE, PROFESSOR_COMPOUND_ROW
)

@pytest.fixture
def sample_truncated_prereq_rows():
    """Multi-row prerequisite that truncates mid-expression."""
    return [COURSE_HEADER_ROW, PREREQ_ROW_TRUNCATED]

@pytest.fixture
def sample_complete_prereq_rows():
    """Multi-row prerequisite that spans continuation and completes properly."""
    return [COURSE_HEADER_ROW, PREREQ_ROW_TRUNCATED, PREREQ_ROW_CONTINUATION]

@pytest.fixture
def sample_professor_text():
    return PROFESSOR_COMPOUND_ROW

@pytest.fixture
def minimal_valid_course():
    """Minimal course dict that satisfies courses JSON schema."""
    return {
        "codigo": "138201",
        "nombre": "Microeconomía I",
        "creditos": "4",
        "prerequisitos": None,
        "secciones": [
            {
                "seccion": "A",
                "docentes": ["CASTROMATTA, Milagros Del Rosario"],
                "observaciones": "",
                "sesiones": [
                    {
                        "tipo": "CLASE",
                        "dia": "LUN",
                        "hora_inicio": "07:30",
                        "hora_fin": "09:30",
                        "aula": "A-301"
                    }
                ]
            }
        ]
    }
```
  </action>
  <verify>
    <automated>cd C:/Users/johnb/Documents/GitHub/MatriculaUp && python -c "import tests.fixtures.sample_rows; print('fixtures OK')"</automated>
    <manual>Verify tests/fixtures/sample_rows.py exists with all 5 named constants</manual>
  </verify>
  <done>tests/conftest.py and tests/fixtures/sample_rows.py exist; `python -c "import tests.fixtures.sample_rows"` exits 0</done>
</task>

<task type="auto">
  <name>Task 2: Write failing unit tests (RED phase)</name>
  <files>
    tests/test_extraction.py
    tests/test_validation.py
  </files>
  <action>
Write tests that MUST fail RED before implementation code exists. The import targets (`scripts.extractors.courses`, `scripts.extractors.validators`) do not exist yet — that is intentional and correct.

`tests/test_extraction.py`:
```python
import pytest
import re

# These imports will FAIL until Plan 02 creates the modules — that is correct RED state
try:
    from scripts.extractors.courses import extract_prerequisites_with_continuation, is_truncated_prerequisite
    from scripts.extractors.courses import extract_professors_spanish
    MODULES_AVAILABLE = True
except ImportError:
    MODULES_AVAILABLE = False

skip_if_no_modules = pytest.mark.skipif(not MODULES_AVAILABLE, reason="Implementation not yet written")


class TestPrerequisiteContinuation:
    """EXT-02: Multi-row prerequisite must be merged complete."""

    @skip_if_no_modules
    def test_truncated_prerequisite_detected(self):
        """Prerequisite ending with 'Y (' must be flagged as truncated."""
        truncated = "138201 Microeconomía I Y ("
        assert is_truncated_prerequisite(truncated) is True

    @skip_if_no_modules
    def test_complete_prerequisite_not_flagged(self):
        """A properly closed prerequisite must NOT be flagged."""
        complete = "138201 Microeconomía I Y (166097 Contabilidad Financiera I)"
        assert is_truncated_prerequisite(complete) is False

    @skip_if_no_modules
    def test_prerequisite_continuation_merges_rows(self, sample_complete_prereq_rows):
        """Multi-row continuation buffer must join rows into single expression."""
        result = extract_prerequisites_with_continuation(sample_complete_prereq_rows)
        # Must have merged continuation, prerequisite must not be truncated
        for course in result:
            prereq = course.get("prerequisitos", {})
            raw = prereq.get("raw", "") if isinstance(prereq, dict) else str(prereq)
            assert not is_truncated_prerequisite(raw), f"Truncated prereq in: {raw}"

    @skip_if_no_modules
    def test_truncated_row_raises_or_flags(self, sample_truncated_prereq_rows):
        """Single-row truncated prerequisite must be detected (raises or sets parsed=False)."""
        result = extract_prerequisites_with_continuation(sample_truncated_prereq_rows)
        for course in result:
            prereq = course.get("prerequisitos", {})
            if isinstance(prereq, dict):
                assert prereq.get("parsed") is False or "raw" in prereq


class TestProfessorSpanishNames:
    """EXT-03: Spanish compound surnames must be captured fully."""

    @skip_if_no_modules
    def test_compound_surname_del(self):
        text = "CASTROMATTA, Milagros Del Rosario"
        result = extract_professors_spanish(text)
        assert len(result) == 1
        assert "Del Rosario" in result[0], f"Expected full name, got: {result[0]}"

    @skip_if_no_modules
    def test_compound_surname_de_la(self):
        text = "GARCIA, Juan De La Cruz"
        result = extract_professors_spanish(text)
        assert len(result) == 1
        assert "De La Cruz" in result[0], f"Expected full name, got: {result[0]}"

    @skip_if_no_modules
    def test_simple_name_unchanged(self):
        text = "SMITH, John"
        result = extract_professors_spanish(text)
        assert len(result) == 1
        assert result[0] == "SMITH, John"

    @skip_if_no_modules
    def test_multiple_professors_split(self, sample_professor_text):
        """Multiple professors separated by ' / ' must each be extracted."""
        result = extract_professors_spanish(sample_professor_text)
        assert len(result) >= 2, f"Expected 2+ professors, got {len(result)}: {result}"
```

`tests/test_validation.py`:
```python
import pytest

try:
    from scripts.extractors.validators import validate_courses_json, validate_curriculum_json
    VALIDATORS_AVAILABLE = True
except ImportError:
    VALIDATORS_AVAILABLE = False

skip_if_no_validators = pytest.mark.skipif(not VALIDATORS_AVAILABLE, reason="Implementation not yet written")


class TestJsonSchemaCompliance:
    """EXT-04: JSON output must validate against defined schema."""

    @skip_if_no_validators
    def test_valid_course_passes(self, minimal_valid_course):
        errors = validate_courses_json({"metadata": {"ciclo": "2026-1", "fecha_extraccion": "2026-02-24"}, "cursos": [minimal_valid_course]})
        assert errors == [], f"Valid course should pass, got: {errors}"

    @skip_if_no_validators
    def test_missing_codigo_fails(self, minimal_valid_course):
        bad = dict(minimal_valid_course)
        del bad["codigo"]
        errors = validate_courses_json({"metadata": {"ciclo": "2026-1", "fecha_extraccion": "2026-02-24"}, "cursos": [bad]})
        assert len(errors) > 0, "Missing 'codigo' should fail schema validation"

    @skip_if_no_validators
    def test_invalid_session_type_fails(self, minimal_valid_course):
        bad = dict(minimal_valid_course)
        bad["secciones"][0]["sesiones"][0]["tipo"] = "UNKNOWN_TYPE"
        errors = validate_courses_json({"metadata": {"ciclo": "2026-1", "fecha_extraccion": "2026-02-24"}, "cursos": [bad]})
        assert len(errors) > 0, "Unknown session type should fail schema validation"

    @skip_if_no_validators
    def test_missing_hora_inicio_fails(self, minimal_valid_course):
        bad = dict(minimal_valid_course)
        del bad["secciones"][0]["sesiones"][0]["hora_inicio"]
        errors = validate_courses_json({"metadata": {"ciclo": "2026-1", "fecha_extraccion": "2026-02-24"}, "cursos": [bad]})
        assert len(errors) > 0, "Missing 'hora_inicio' should fail"
```
  </action>
  <verify>
    <automated>cd C:/Users/johnb/Documents/GitHub/MatriculaUp && pip install pytest --quiet && pytest tests/test_extraction.py tests/test_validation.py --collect-only -q 2>&1 | head -30</automated>
    <manual>Confirm test collection shows 10+ tests collected. All skip (because modules don't exist yet) or show ImportError — that is correct RED state.</manual>
  </verify>
  <done>pytest collects 10+ tests from test_extraction.py and test_validation.py; tests are in skipped or error state (not passing) because implementation modules don't exist yet</done>
</task>

</tasks>

<verification>
Run: `cd C:/Users/johnb/Documents/GitHub/MatriculaUp && pytest tests/ --collect-only -q`
Expected: 10+ tests collected, all skipped or error (no false green). Confirms RED baseline.
</verification>

<success_criteria>
- tests/conftest.py, tests/fixtures/sample_rows.py, tests/test_extraction.py, tests/test_validation.py all exist
- pytest collects tests without syntax errors
- No tests pass GREEN yet (modules don't exist) — this is correct
- requirements.txt includes pytest>=8.0.0 and pytest-cov>=5.0.0
</success_criteria>

<output>
After completion, create `.planning/phases/01-extraction-pipeline-fix-validation/01-01-SUMMARY.md`
</output>
